---
title: "Group Project_Notes"
author: "Silas and Yusuf"
date: "2025-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 11/11/25

### Today we got a shared directory for file management that lives in: 
`/gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData`

### Our data comes from NCBI SRA: 

### SRR4102063
### SRR4102065

##Inputting Data 

### We first had to load the required modules: 
```
module load gcc/13.3.0-xp3epyt

module load sratoolkit/3.0.0-y2rspiu
```

### We then configured the VACC ~ SRA relationship:
```{}
vdb-config --interactive
```
### We really got stuck on finding data that was parsed out based on replicates. This took nearly all the in time class for the project. 
### Then we used the prefetch command in terminal: 
```{}
prefetch SRR4102062 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData


prefetch SRR4102063 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR4102064 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR4102065 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR23936509 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

```
### This now provides SRA files in our folder, we need to make them FASTQ files: 

```
fasterq-dump SRR4102063 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

fasterq-dump SRR4102065 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

# Try with new data for reps 
fasterq-dump SRR23936509 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData
```
### We now are attempting to compress them to .gz files: 
```
gzip SRR4102063_1.fastq
gzip SRR4102063_2.fastq
gzip SRR23936509_1.fastq
gzip SRR23936509_2.fastq



```
## 11/13/25

### First mission was to succesfully compress our fastq files into gzips via: 

```
gzip SRR4102063_1.fastq
gzip SRR4102063_2.fastq
gzip SRR4102065_1.fastq
gzip SRR4102065_2.fastq


```
# Barcode Identification 

```
zcat SRR4102063_1.fastq.gz | awk 'NR%4==2{print substr($0,1,6)}' | sort | uniq -c | sort -nr | head -50
```

### look into Fastcat to look into indexs for different tomates in the pool 

```
fascat --help
```
# 11/20/2025 
## Time to get down to business - here on out consists of the actual data we will be using 

### We finally found some data that contained individual SRA for each replicate 

## Samples: 3x hydroponic grown crysanthenum (Crys) and 3x soil grown # SRR15057665	hydroponic 1 
# SRR15057666	hydroponic 2 
# SRR15057667	hydroponic 3
# SRR15057668	soil 1 
# SRR15057669	soil 2 
# SRR15057670	soil 3 

## Pulling in SRA Files 
### A bash script was made 'SRA_pull_GP.sh' to run these simultanesouly and it lives in : 

`/gpfs1/home/s/m/smdecker/projects/eco_genomics_2025/transcriptomics/myscripts`

```{}

prefetch SRR15057665 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR15057666 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR15057667 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR15057668 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR15057669 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

prefetch SRR15057670 --output-directory /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

```

## Creating Fastq Files 

```
fasterq-dump SRR15057665 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

fasterq-dump SRR15057666 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

# Try with new data for reps 
fasterq-dump SRR15057667 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

fasterq-dump SRR15057668 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

fasterq-dump SRR15057669 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

fasterq-dump SRR15057670 --split-files --outdir /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData

```
## Compressing Fastq to .gz

```

gzip SRR15057665_1.fastq
gzip SRR15057665_2.fastq
gzip SRR15057666_1.fastq	
gzip SRR15057666_2.fastq	 
gzip SRR15057667_1.fastq
gzip SRR15057667_2.fastq	
gzip SRR15057668_1.fastq
gzip SRR15057668_2.fastq	 
gzip SRR15057669_1.fastq
gzip SRR15057669_2.fastq	
gzip SRR15057670_1.fastq
gzip SRR15057670_2.fastq


```
### About 10 times the gzip script failed on VACC. I thus spoke with AI for a while to make a succinct looped script to compress fastq files that it tited 'looped_gzip.sh' and lives in:

`/gpfs1/home/s/m/smdecker/projects/eco_genomics_2025/transcriptomics/myscripts`

## Cleaning Data 

## We made a script for running fastp names 'Chrys_fastp_GP.sh' that lives in : 
`/gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData`

### The quality is above 35 more or less, which is excellent

## 11/21/25 

## Map reads with genome via STAR

### Today I am attempting to map our cleaned transcriptomic reads to a reference *genome* in hopes that *it maps well

### I am using a *C. seticuspe* reference which is hopefully closely related to the species (not stated) in our paper 

### Reference genome: GCA_019973895.1 

### Downloading the reference is being a pain in the ___. My pc does not have the space to download so I am trying to figure out how to upload to VACC. 


## 11/23/25 **Actual** Uploading reference genome 

### Same reference as from 11/21, I purged a lot of files from my computer to make room, the download files followed by scp is much easier

### Made a text file 'genome_transfer_GP.txt' which serves as the scripts to input into the terminal.
### This script uses scp from local machine (windows powershell) to send the downloaded (GBF for annotations and fna for genome) to the VACC folder for this group project: 
`/gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData`

### The gbf file then needs to be converted to a gtf file for input to star (code on same script as scp commands - genome_transfer_GP.txt)


## 11/24/25

### It appears that one of the STAR mapping runs didn't work well
### Two of the 6 files mapped produced 0 byte bam files

### I now realize I mapped both cleaned and raw fastq, so overwriting certainly happened. 

### I am now making a folder in RawData for clean reads, and will rerun STAR mapping for only clean reads. Please, Please work on 6/6 bash gods

### I am making the same script as mapped yesterday, but will be using only clean fastqs which are stored in a new place: 
`cd /gpfs1/cl/ecogen/pbio6800/GroupProjects/SpicyTomates/RawData/fastp_clean`

### New mapping script: STAR_mapping_clean.sh lives in : 

`/gpfs1/home/s/m/smdecker/projects/eco_genomics_2025/transcriptomics/myscripts`

### Wow, this is really stressful. I think I have to re-index STAR since apparently the hours spent running that yesterday didn't work



